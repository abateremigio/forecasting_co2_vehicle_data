---
title: "Forecasting CO2 Emissions Using Vehicle Data"
author: "Matteo Frison"
date: "February 2025"
output:
  pdf_document: 
     keep_tex: true
subtitle: |
  | Project submitted for the course Data Analytics I
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(GGally)
library(corrplot)
library(patchwork)
library(glmnet)
library(grf)
library(randomForest)
library(caret)
library(xgboost)
library(future.apply)
library(doParallel)
library(foreach)

path = "/Users/matteofrison/Library/CloudStorage/OneDrive-UniversitaetSt.Gallen/Coursework/Data Analytics I/Assignments/Individual Assignment/"
```

This is a data analytics and machine learning project that aims to predict the CO2 emissions of motor vehicles. It analyzes the impact of features such as engine size, number of cylinders, and fuel consumption on CO2 emissions. The project begins with exploratory data analysis, to understand the key data characteristics. Then, Lasso, Post-Lasso, Random Forest and Gradient Boosting models are trained and fine-tuned to predict emissions. Finally, the performance of the models is evaluated using accuracy metrics.

# Data Cleaning and Exploratory Data Analysis:
The training data comes in a Rdata file (`co2.Rdata`) and contains information on vehicle specifications, fuel consumption, and CO~2~ emissions. In particular, the dataset contains the following variables:

- `Make`: The brand or manufacturer of the vehicle (e.g., Toyota, Ford, BMW)
- `Vehicle.Class`: The class of the vehicle:
  - `COMPACT`: Smaller-sized vehicles
  - `FULL-SIZE`: Larger-sized vehicles
  - `MID-SIZE`: Medium-sized vehicles
  - `MINICOMPACT`: Very small-sized vehicles
  - `MINIVAN`: Smaller-sized vans
  - `PICKUP TRUCK - SMALL`: Smaller-sized pickup trucks
  - `PICKUP TRUCK - STANDARD`: Standard-sized pickup trucks
  - `SPECIAL PURPOSE VEHICLE`: Vehicles designed for special purposes
  - `STATION WAGON - MID-SIZE`: Mid-sized station wagons
  - `STATION WAGON - SMALL`: Smaller-sized station wagons
  - `SUBCOMPACT`: Smaller than compact-sized vehicles
  - `SUV - SMALL`: Smaller-sized sports utility vehicles
  - `SUV - STANDARD`: Standard-sized sports utility vehicles
  - `TWO-SEATER`: Vehicles with two seats
  - `VAN - CARGO`: Vans designed for cargo
  - `VAN - PASSENGER`: Vans designed for passenger transportation
- `Engine.Size.L`: The engine size in liters
- `Cylinders`: The number of cylinders in the engine
- `Transmission`: The type of transmission:
  - `A`: Automatic. A transmission type that operates without the need for the driver to manually change gears
  - `AM`: Automated manual. A version of a manual transmission that is automated
  - `AS`: Automatic with select shift. An automatic transmission that allows for manual intervention
  - `AV`: Continuously variable. A transmission that uses continuously varying ratios instead of fixed gear ratios (like e.g. 5)^[Continuously variable transmissions (CVTs) do not have fixed gears by their nature. However, to give drivers a more familiar driving experience (similar to traditional automatic transmissions that shift through gears), some CVTs are programmed to simulate shifting gears.]
  - `M`: Manual. A transmission type that requires the driver to manually change gears
  - `4 - 10`: Number of gears in the transmission.
- `Fuel.Type`: The type of fuel used:
  - `X`: Regular gasoline
  - `Z`: Premium gasoline
  - `D`: Diesel
  - `E`: Ethanol (E85)
  - `N`: Natural gas
- `Fuel.Consumption.City.L.100.km`: Fuel consumption in the city (liters per 100 kilometers)
- `Fuel.Consumption.Hwy.L.100.km`: Highway (out-of-city) fuel consumption (liters per 100 kilometers)
- `Fuel.Consumption.Comb.L.100.km`: Combined (city and highway) fuel consumption (liters per 100 kilometers)
- `Fuel.Consumption.Comb.mpg`: Combined fuel consumption (miles per gallon)
- `CO2.Emissions.g.km`: CO2 emissions in grams per kilometer

In addition, I have a separate dataset for prediction only, from which the ground truth was removed. I start by loading the training and test data in my environment:
```{r, cache=TRUE}
# Load the data
load(paste0(path, "co2.Rdata"))
load(paste0(path, "co2_predict.Rdata"))

print(colnames(co2))
```

1. The levels of `Vehicle.Class` are long and sometimes include forbidden characters (spaces and minus signs). I recode them for convenience.
```{r}
# List variables by type
categorical_vars = c("Make", "Vehicle.Class", "Transmission", "Fuel.Type")
numerical_vars = c("Engine.Size.L", "Cylinders", "Fuel.Consumption.City.L.100.km", "Fuel.Consumption.Hwy.L.100.km",
                   "Fuel.Consumption.Comb.L.100.km", "Fuel.Consumption.Comb.mpg", "CO2.Emissions.g.km")

# Define recoding
vehicle_class_recode = c("COMPACT" = "Compact", "SUV - SMALL" = "SUV.Small", "MID-SIZE" = "Midsize",
    "TWO-SEATER" = "Two.Seater", "MINICOMPACT" = "Minicompact", "SUBCOMPACT" = "Subcompact",
    "FULL-SIZE" = "Full.Size", "STATION WAGON - SMALL" = "Station.Wagon.Small", 
    "SUV - STANDARD" = "Standard.SUV", "VAN - CARGO" = "Van.Cargo",
    "VAN - PASSENGER" = "Van.Passenger", "PICKUP TRUCK - STANDARD" = "Pickup.Standard",
    "MINIVAN" = "Van.Small", "SPECIAL PURPOSE VEHICLE" = "Special.Purpose",
    "STATION WAGON - MID-SIZE" = "Station.Wagon.Mid", "PICKUP TRUCK - SMALL" = "Pickup.Small")

# Apply to training and test set
co2 = co2 %>% mutate(Vehicle.Class = recode(Vehicle.Class, !!!vehicle_class_recode))
co2_predict = co2_predict %>% mutate(Vehicle.Class = recode(Vehicle.Class, !!!vehicle_class_recode))
```

2. I proceed to convert the categorical variables into numerical ones, by target mean encoding, so that the entire dataset can be analyzed together. I do this by replacing each category with the mean of the response variable (`CO2.Emissions.g.km`) for that category. Since some of the levels of the categorical variables do not appear in both the training and the test sets, if the test set has a category that does not exist in the training set, I assign it the global mean of `CO2.Emissions.g.km` from the training set. This proceure might lead to some overfitting, especially for the categories for which we only have a few observations. It is also not optimal because there are some categories that are present in the test set but not in the training data. However, I believe that target encoding is still better than a random encoding or not using the categorical predictors.
```{r}
# Mean encoding function
mean_encode = function(train_data, test_data, var, response) {
  # Compute mean encoding from training data
  encoding = train_data %>%
    group_by(!!sym(var)) %>%
    summarise(mean_value = mean(!!sym(response))) %>%
    ungroup()
  
  # Replace cat vars in train and test sets
  train_data = train_data %>%
    left_join(encoding, by = var) %>%
    select(-!!sym(var)) %>%
    rename(!!var := mean_value)
  
  test_data = test_data %>%
    left_join(encoding, by = var) %>%
    select(-!!sym(var)) %>%
    rename(!!var := mean_value)

  # Unseen categories in test set are assigned sample mean
  sample_mean = mean(train_data[[var]], na.rm = TRUE)
  test_data[[var]][is.na(test_data[[var]])] = sample_mean

  return(list(train_data = train_data, test_data = test_data))
}

co2_numeric = co2
co2_predict_numeric = co2_predict

for (var in categorical_vars) {
  # Apply function to the cat vars
  encoded = mean_encode(co2_numeric, co2_predict_numeric, var, "CO2.Emissions.g.km")
  # Update the data with encoding
  co2_numeric = encoded$train_data 
  co2_predict_numeric = encoded$test_data
}
```

3. I check the dimensions of the datasets and, the presence of NAs and the number of observations per each categorical variable, to find that there are no NAs. However, there seems to be some categories with a very small number of observations (e.g. BUGATTI, SMART and SRT under `Brand`; AM5, AM9, AS4, under `Transmission`, and Natural.Gas under `Fuel.Type`).
```{r}
# Verify dataset dimensions
cat("Training set dimensions:", dim(co2))
cat("\nNumber of NAs:",sum(is.na(co2)))

cat("\nTest set dimensions:", dim(co2_predict))
cat("\nNumber of NAs:",sum(is.na(co2_predict)))

# Verify the # of observations for each categorical variable
for (var in categorical_vars) {
  cat("\nCount for", var, "in training set:\n")
  print(table(co2[[var]]))
  cat("\nCount for", var, "in test set:\n")
  print(table(co2_predict[[var]]))
}
```

4. I scatterplot all the variables against each other, plot their frequency distributions and check their correlation. The last row of the pair plot shows that CO2 emissions are loosely linear with respect to the predictors, except for `Make`, `Vehicle.Class` `Transmission`, `Fuel.Type` and `Fuel.Con.Comb.mpg`. The reason for non-linearity in these predictors seem to be a missing quadratic term for `Fuel.Con.Comb.mpg`, but is less clear for the others. From the correlation plots, we can see that all the predictors are positively correlated with `CO2.Emissions.g.km`, except for `Fuel.Con.Comb.mpg`, which is intuitively correct. No significant differences is observable in the correlation plots between the training and the test sets.
```{r}
# Create the pair plots
ggpairs(co2_numeric, title = "Pair Plot of trainind data", progress = FALSE)
ggpairs(co2_predict_numeric, title = "Pair Plot of test data", progress = FALSE)

# Plot correlation matrices
corrplot(cor(co2_numeric), method = "color", type = "upper", order = "hclust",
         addCoef.col = NULL, tl.col = "black", tl.srt = 45)
corrplot(cor(co2_predict_numeric), method = "color", type = "upper", order = "hclust",
         addCoef.col = NULL, tl.col = "black", tl.srt = 45)
```

5. I finish my exploratory data analysis and data cleaning by creating boxplots of the categorical variables. 
  - From the `Brand` boxplot, we can observe that certain brands, such as TOYOTA, LEXUS, FORD, CHEVROLET and BMW produce both high and low-emission cars, while brands such as LAMBORGHINI, ASTON MARTIN or BUGATTI seem to specialize on high-emission vehicles. 
  - From the `Vehicle.Class` boxplot we can see that emissions correlate with size: generally bigger vehicles emit more CO2.
  - The type of `Transmission` does not seem to explain emissions, as there is a large overlap in the interquartile ranges of manual and automatic cars. However, emissions seem to be proportional to the number of gears.
  - `Fuel.Type` does not seem to significantly influence emissions. Likely, the differences in the boxplots are due to omitted variables.
  
6. Finally, I create a test set out of the training data, to evaluate the three approaches and choose the best in the end.
```{r}
# Boxplot categorical variables
for (i in seq_along(categorical_vars)) {
  cat_var = categorical_vars[i]
  co2[[cat_var]] = reorder(co2[[cat_var]], co2$CO2.Emissions.g.km, mean)
  
  boxplot(CO2.Emissions.g.km ~ co2[[cat_var]], data = co2,
          main = paste("CO2 Levels by", gsub("\\.", " ", cat_var)),
          xlab = cat_var,
          ylab = "CO2 Levels",
          las = 2,
          col = rainbow(length(categorical_vars))[i])
}

# Reorder vars by type
catnum = c(categorical_vars, numerical_vars)
co2_numeric = co2_numeric[, catnum]
co2_predict_numeric = co2_predict_numeric[, catnum[1:10]]

# Create a test set (10% of the training data)
test.indexes = sample(x = 1:nrow(co2_numeric), 
                            size = round(nrow(co2_numeric)*0.1),
                            replace = FALSE)

test.set = co2_numeric[test.indexes, ]
co2_numeric = co2_numeric[-test.indexes, ]
```

# Lasso and Post Lasso
 *Method*: CO~2~ is a continuous numerical variable, and, from the scatter plots, we see that it appears linear or quadratic in the numerical predictors, except for `Fuel.Consumption.Comb.mpg`, which seems to have a quadratic relation with the response. In light of this, my first approach consists in using the all the predictors in a Lasso regression to select the best and use them in a post Lasso, OLS regression, where I can model the quadratic factor for `Fuel.Consumption.Comb.mpg`. Lasso regression is a regularization method that adds a L1 norm penalty term to the OLS cost function. Because of the geometry of the penalty, Lasso allows to perform features selection. 
 
Hereafter, I convert the training data to matrix type, the input format of the `cv.glmnet` function. Since it also performs k-fold cross validation I do not partition the data in training and validation sets.

*Tuning*: Lasso tuning parameter is `lambda`. I chose it through 5-fold cross-validation.
```{r approach1, cache=TRUE}
set.seed(123)

# Matrices for Lasso
X = as.matrix(co2_numeric[, c(1:10)])
Y = co2_numeric$CO2.Emissions.g.km

# Run lasso regression choosing lambda with 5-fold CV
lasso.model = cv.glmnet(X, Y, type.measure = "mse", 
                        family = "gaussian", nfolds = 5, alpha = 1)

coef.lasso.model = coef(lasso.model, s = "lambda.min") # Extract coeffs
print(coef.lasso.model)

# Calculate the MSE for lasso model
# Predict CO2 emissions
pred.lasso = predict(lasso.model, newx = as.matrix(test.set[1:10]), 
                            s = lasso.model$lambda.min)

# Compute the MSE
predMSElasso = mean((test.set[ ,11] - pred.lasso)^2)
cat("MSE lasso:", predMSElasso)

# Calculate the R2 for the post lasso model
R2lasso = 1 - sum((test.set[, 11] - pred.lasso)^2)/sum((test.set[, 11] - mean(test.set[, 11]))^2)
cat("\nR2 lasso:", R2lasso)

# Post Lasso
post.lasso.vars = rownames(coef.lasso.model)[coef.lasso.model[, 1] != 0]  # Select nonzero coefficients
post.lasso.vars = setdiff(post.lasso.vars, "(Intercept)")  # Remove intercept

# Subset data with lasso-selected predictors
co2.post.lasso = co2_numeric[, post.lasso.vars, drop = FALSE]

# Fit OLS model using only selected variables
post_lasso_model = lm(co2_numeric$CO2.Emissions.g.km ~ . + I(Fuel.Consumption.Comb.mpg^2), 
                      data = co2.post.lasso)

# Display summary
summary(post_lasso_model)

# Predict CO2 emissions
pred.post.lasso = predict(post_lasso_model, newdata = test.set[, 1:10])

# Calculate the MSE
predMSEposlasso = mean((test.set[, 11] - pred.post.lasso)^2)
cat("MSE post-lasso:", predMSEposlasso)

# Calculate the R2
R2postlasso = 1 - sum((test.set[, 11] - pred.post.lasso)^2)/sum((test.set[, 11] - mean(test.set[, 11]))^2)
cat("\nR2 post-lasso:", R2postlasso)
```

# Regression Forest
1. *Method*: random forests combine multiple decision trees to improve prediction accuracy and reduce overfitting. It works by training each tree on a random subset of the data (using bootstrap sampling) and making decisions based on a random subset of features. The final prediction, in case of regression forests, is obtained by averaging. This produces decorrelated trees and leads to a lower prediction variance. 

2. *Partition and further transformations*: I do not partition the data into validation and test set. The convenient feature of forest is the possibility to compute the out-of-bag estimate of the test error. The OOB error is obtained by evaluating each tree on the subset of training data that was not used in its construction. This provides a built-in mechanism for model validation without requiring an explicit hold-out validation set. 

Again, I apply the same matrix transformation I applied for the Lasso model.

```{r}
set.seed(111)
# Matrix setup
X = as.matrix(co2_numeric[, 1:10])
Y = as.numeric(as.matrix(co2_numeric$CO2.Emissions.g.km))
newX = as.matrix(test.set[, 1:10]) # Prediction data

# Grow Forest
forest.untuned = randomForest(x = X, y = Y, ntree = 500, do.trace = 0)
print(forest.untuned)

# Plot the out-of-bag error per number of trees
plot(forest.untuned$mse, type = 'b', pch = 19, col = 'red',
     xlab = 'Number of Trees', ylab = 'OOB Estimate of Test Error',
     main = 'OOB Estimate of Test Error vs. Number of Trees')
abline(h = min(forest.untuned$mse))
```

3. *Tuning*: random forests have 3 tuning parameters:
- `mtry`: the number of variables considered in the tree growth procedure,
- `nodesize`: minimum leaf size (governs tree depth),
- `ntree`: the number of trees.

I start by growing a random forest with the default parameters. In the RF algorithm, default values usually work quite well, but we can use the out-of-bag estimate of the test error to determine the optimal values of the tuning parameters. I use the `randomForest` package, instead of `grf`, because it makes it easier to plot the out-of-bag error for each number of trees. The plot shows that after 250 trees, the OOB error stabilizes. 

To tune `mtry` and `nodesize`, hereafter I consider a grid of 5 values for each of these parameters and combine them into an overall grid. The intervals in which these 5 values are located are chosen so that they contain the initial default parameters of the untuned forest. For the `ntree` parameter, it is sufficient to consider only the highest value; for the other values, I use the trajectory of the out-of-bag estimate obtained by growing a forest for each combination of the parameters' values.

```{r}
# Generate grid of parameters
mtry.grid = seq(from = 1, by = 1, length.out = 5)
nodesize.grid = seq(from = 1, by = 5, length.out = 5)
ntree.grid = seq(from = 300, by = 150, length.out = 5)

rf.pars.grid = expand.grid(mtry.grid, nodesize.grid)

# Parallelize the computation on 5 CPU cores to speed up
no.cores = 5
cl = makeCluster(no.cores, outfile = "log.txt")
clusterExport(cl, c("rf.pars.grid", "X", "Y", "ntree.grid", "randomForest"))

# Define a function that grows RF based on grid values
runParsValue.rf = function (i.pars) {
   cat('i.pars = ', i.pars, '\n')
   mtry = rf.pars.grid[i.pars, 1]
   nodesize = rf.pars.grid[i.pars, 2]

   set.seed(111)
   rf = randomForest(x = X, y = Y,
                     mtry = mtry,
                     nodesize = nodesize,
                     ntree = max(ntree.grid))

   return(c(mtry, nodesize, rf$mse[ntree.grid]))
 }

res.rf.tuning = parSapply(cl, 1:nrow(rf.pars.grid), runParsValue.rf)
stopCluster(cl)
 
rownames(res.rf.tuning) = c('mtry', 'nodesize', as.character(ntree.grid))
t(res.rf.tuning)

# Organize these results into a three-dimensional array
a = array(data = c(t(res.rf.tuning[3:7, ])),
          dim = c(5, 5, 5),
          dimnames = list(mtry = mtry.grid,
                          nodesize = nodesize.grid,
                          ntree = ntree.grid))

# Extrct the best parameters
min.pos = which(a == min(a), arr.ind = TRUE)
mtry.opt = mtry.grid[min.pos[1]]
nodesize.opt = nodesize.grid[min.pos[2]]
ntree.opt = ntree.grid[min.pos[3]]

# Build the forest corresponding to optimal parmeters
set.seed(111)
tuned.forest = randomForest(x = X, y = Y,
                                 mtry = mtry.opt, nodesize = nodesize.opt,
                                 ntree = ntree.opt)
 
# Predict CO2 emissions
pred.forest.tuned = predict(tuned.forest, newdata = newX)

# Calculate MSE
predMSEtuned.forest = mean((test.set[ ,11] - as.matrix(pred.forest.tuned))^2)
cat("\nMSE tuned forest:", predMSEtuned.forest)

# Calculate the R2
R2tuned_forest = 1 - sum((test.set[, 11] - as.matrix(pred.forest.tuned))^2)/sum((test.set[, 11] - mean(test.set[, 11]))^2)
cat("\nR2 tuned forest:", R2tuned_forest)
```

# Gradient Boosting
1. *Method*: Gradient Boosting regression (XGBoost package) is an ensemble method that builds multiple decision trees sequentially (not independently like in random forest), where each new tree corrects the errors of the previous ones. It uses gradient boosting with regularization techniques to improve accuracy and prevent overfitting. Like random forests, XGBoost handles well the complex/nonlinear relationships in data. XGBoost is more computationally expensive than random forest (tuning might take a while), due to sequential learning, but can sometimes yield a lower test MSE.

The XGBoost algorithm starts with an initial guess, then calculates the residuals between actual values and predicted values. A new decision tree is trained to predict the residuals from the previous step. The predictions are updated by adding a share of the new treeâ€™s predictions (controlled by eta). These steps are repeated until the RMSE hits a stopping rule.

2. *Partition and further transformations*: Instead of training the model on the full training set, I randomly created a validation set not used to train the model, but just to estimate the test error at each step of the training. There is clearly a tradeoff between the sizes of the "true" training set and of the evaluation set. Here I use 10% of the training sample observations for validation purposes. Since the `xgboost` package requires the training and validation sets to be `xgb.DMatrix` objects, I transformed them accordingly.

```{r}
# Split 90:10 training and validation sets
set.seed(123)
validation.indexes = sample(x = 1:nrow(co2_numeric), 
                            size = round(nrow(co2_numeric)/10),
                            replace = FALSE)

dtrain = xgb.DMatrix(data = X[-validation.indexes, ],
                      label = Y[-validation.indexes])
dvalid = xgb.DMatrix(data = X[validation.indexes, ],
                      label = Y[validation.indexes])

# Model estimation using default parameters
watchlist = list(train = dtrain, validation = dvalid)

set.seed(123)
default.xgb.reg.fit = xgb.train(objective = 'reg:squarederror', 
                                data = dtrain,
                                nrounds = 100,
                                watchlist = watchlist, 
                                verbose = 0)
print(default.xgb.reg.fit)
```
3. *Tuning*: I tune the following XGBoost parameters
- `eta`: learning rate. Controls the step size at each boosting iteration. Smaller values make training slower but generalize better. Lower value for eta implies larger value for `nrounds` (default: 0.3).
- `max_depth`: maximum depth of a tree (default: 6),
- `subsample`: fraction of training samples used per boosting round. Prevents overfitting (default: 1),
- `colsample_bytree`: fraction of features randomly selected per tree (default: 1),
- `min_child_weight`: minimum sum of instance weights (hessian) needed in a child node. If the tree partition step results in a leaf node with the sum of instance weight less than `min_child_weight`, then the building process will give up further partitioning. (default: 1),
- `nrounds`: max number of boosting iterations.

Instead of adopting a brute force approach, as I did with the random forest, I implement a random search procedure with 100 attempts. For each attempt, I randomly generate all the parameter values, except for `nrounds`, which is chosen by 5-fold cross-validation. Also, an early stopping criterion applies: I stop when test RMSE does not improve for 10 consecutive rounds. Finally, I extract the parameters' values for the tuned model that minimize the validation RMSE.

```{r}
# Parallelize the computation on all cores but 1
num_cores = detectCores(logical = TRUE) - 1
cl = makeCluster(num_cores)
registerDoParallel(cl)

# Update drtain
dtrain = xgb.DMatrix(data = X, label = Y)

# Create a 5-fold CV object
foldIndexes = createFolds(co2_numeric$CO2.Emissions.g.km, k = 5, list = TRUE, returnTrain = FALSE)

# Convert to xgboost format
folds_xgb = lapply(foldIndexes, function(x) setdiff(1:nrow(co2_numeric), x))
 
set.seed(123)
attempts = 100
res.boost.tuning = matrix(data = NA, nrow = attempts, ncol = 6+1)
colnames(res.boost.tuning) = c('eta', 'max_depth', 'subsample', 
                                'colsample_bytree', 'min_child_weight', 
                                'nrounds', 'testRmse')

 for (i in 1:attempts) {
   # Generate random parameters
   eta = runif(n = 1, min = 0.2, max = 0.4)
   max_depth = sample(x = 5:9, size = 1)
   subsample = runif(n = 1, min = 0.8, max = 1)
   colsample_bytree = runif(n = 1, min = 0.7, max = 1)
   min_child_weight = runif(n = 1, min = 1, max = 1.5)
   
   # Cross validate 
   u = xgb.cv(params = list(objective = 'reg:squarederror',
                            eta = eta,
                            max_depth = max_depth,
                            subsample = subsample,
                            colsample_bytree = colsample_bytree,
                            min_child_weight = min_child_weight,
                            nthread = 5),
              data = dtrain,
              nrounds = 1000,
              metrics = 'rmse',
              early_stopping_rounds = 10,
              folds = folds_xgb,
              verbose = F)
   
   nrounds = which.min(u$evaluation_log$test_rmse_mean)
   testRmse = min(u$evaluation_log$test_rmse_mean)
   results = c(eta, max_depth, subsample, 
               colsample_bytree, min_child_weight, 
               nrounds, testRmse)
   cat(i, results, '\n')
   res.boost.tuning[i, ] = results
 }

# Stop the parallel cluster
stopCluster(cl)

# Extract the best model parameters
indBestModel = which.min(res.boost.tuning[, 'testRmse'])

etaBest = res.boost.tuning[indBestModel, 1]; etaBest
max_depthBest = res.boost.tuning[indBestModel, 2]; max_depthBest
subsampleBest = res.boost.tuning[indBestModel, 3]; subsampleBest
colsample_bytreeBest = res.boost.tuning[indBestModel, 4]; colsample_bytreeBest
min_child_weightBest = res.boost.tuning[indBestModel, 5]; min_child_weightBest
nroundsBest = res.boost.tuning[indBestModel, 6]; nroundsBest

# Run the best xgboost model on the entire co2 set
watchlist = list(train = dtrain)
set.seed(111)
tuned.xgboost = xgb.train(params = list(objective = 'reg:squarederror',
                                            eta = etaBest,
                                            max_depth = max_depthBest,
                                            subsample = subsampleBest,
                                            colsample_bytree = colsample_bytreeBest,
                                            min_child_weight = min_child_weightBest),
                              data = dtrain,
                              nrounds = nroundsBest,
                              watchlist = watchlist,
                              verbose = 0)

# Make predictions on test data
pred_xgboost_tuned = predict(tuned.xgboost, newX)

# Calculate MSE
predMSE_xgboost_tuned = mean((test.set[, 11] - as.matrix(pred_xgboost_tuned))^2)
cat("\nMSE tuned xgboost:", predMSE_xgboost_tuned)

# Calculate the R2
R2xgboost_tuned = 1 - sum((test.set[, 11] - as.matrix(pred_xgboost_tuned))^2)/sum((test.set[, 11] - mean(test.set[, 11]))^2)
cat("\nR2 tuned xgboost:", R2xgboost_tuned)
```

# Performance Comparison and Conlcusion
My preferred approach is the tuned XGboost regression, as it yielded the lowest test MSE and the highest test R-squared. I tried other ways to select the tuning parameters for all my methods:

1. *Lasso and post-lasso* tuning does not improve test MSE when chosen with 10-fold CV or train-validation split.
2. *Random forest* can be tuned differently, by selecting different intervals for the grid search. I tried expanding the grid dimension from 5 to 10, but the tuned model did not improve significantly. I chose to keep 5 to keep execution time reasonably low.
3. *XGboost* can be tuned alternatively by changing the distributions from which the random search process extracts the parameters' values. Better results could be probably achieved by increasing the number of attempts. However, 100 is already high and increasing it is very computationally expensive.
```{r}
models = c("Lasso", "Post Lasso", "Tuned Forest", "Tuned XGBoost")

predMSE = c(predMSElasso, predMSEposlasso, 
            predMSEtuned.forest, predMSE_xgboost_tuned)

R2 = c(R2lasso, R2postlasso, 
       R2tuned_forest, R2xgboost_tuned)

df_mse = data.frame(Model = models, Value = predMSE)
df_r2 = data.frame(Model = models, Value = R2)

comparison = cbind(df_mse, df_r2[, 2])
colnames(comparison) = c("Model", "Test MSE", "Out-of-Sample R2")
print(comparison)

# MSE comparison
ggplot(df_mse, aes(x = Model, y = Value, fill = "MSE")) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Test MSE by Model", x = "Model", y = "Test MSE") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

# R2 comparison
ggplot(df_r2, aes(x = Model, y = Value, fill = "R2")) +
  geom_bar(stat = "identity", fill = "darkorange") +
  labs(title = "Out-of-sample R2 by Model", x = "Model", y = "R2") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") +
  coord_cartesian(ylim = c(0.8, 1))
```